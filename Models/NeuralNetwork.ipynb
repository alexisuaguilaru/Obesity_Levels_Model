{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {},
   "source": [
    "This notebook contains the procedure and decisions taken to create a neural network using [PyTorch](https://pytorch.org/) for the classification of the obesity level of patients based on their lifestyle habits.\n",
    "\n",
    "In section [1. Load Dataset](#1-load-dataset) the training and evaluation datasets of the model are loaded making use of the functionalities offered by PyTorch.\n",
    "\n",
    "In section [2. Model Architecture](#2-model-architecture) the neural network for the classifier is defined, with a focus on being a simple and small network but capable of learning to classify obesity levels correctly.\n",
    "\n",
    "Finally, in section [3. Model Training](#3-model-training) the parameters (weights and bias) of the model defined in the previous section are optimized using the optimizer [Adam](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html) and [CrossEntropyLoss](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {},
   "source": [
    "Subclassing of [`Dataset`](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is used to load the dataset so that it can be compatible with the neural networks in PyTorch. The [`DataLoader`](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) used to train and evaluate the models are created in the `Trainer`. When loading the dataset, no normalization or scaling transformation is applied to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing auxiliar libraries\n",
    "\n",
    "import marimo as mo\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "from functools import partial\n",
    "from torch import nn , device , optim , Tensor\n",
    "from torch.cuda import is_available\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Importing Functions and Utils\n",
    "\n",
    "import SourceModels as src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining useful variables\n",
    "\n",
    "PATH = './'\n",
    "PATH_SAVE = PATH + 'SaveModels/'\n",
    "\n",
    "NUM_JOBS = src.GetNumJobs()\n",
    "TORCH_DEVICE = device('cuda' if is_available() else 'cpu')\n",
    "\n",
    "RANDOM_STATE = 8013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting features \n",
    "\n",
    "DatasetFilename = PATH + 'Dataset_{}.csv'\n",
    "_Dataset = pd.read_csv(DatasetFilename.format('Train'),nrows=1)\n",
    "\n",
    "NumericalFeatures , CategoricalFeatures , Target = src.SplitFeatures(_Dataset)\n",
    "Features = [*NumericalFeatures,*CategoricalFeatures]\n",
    "\n",
    "del _Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasets\n",
    "\n",
    "Dataset_Train: Dataset = None\n",
    "Dataset_Evaluation: Dataset = None\n",
    "for _type_dataset in ['Train','Evaluation']:\n",
    "    globals()[f'Dataset_{_type_dataset}'] = src.DatasetLoader(\n",
    "        DatasetFilename.format(_type_dataset),\n",
    "        Features,\n",
    "        Target,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {},
   "source": [
    "It makes use of a simple architecture with two hidden layers of $40$ and $25$ neurons, respectively, with `ReLU` activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Neural network architecture for \n",
    "        predicting the obesity level of \n",
    "        a person\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Linear(21,40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40,25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25,7),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            Instance_X: Tensor\n",
    "        ) -> Tensor:\n",
    "\n",
    "        Logits = self.NN(Instance_X)\n",
    "        return Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "For the training of the neural network it was decided to use the optimizer [Adam](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html), due to its robustness generated by its adaptive learning mechanisms, with a learning rate of $0.01$ and $50$ epochs with batch size of $32$ because there are not enough training instances (this setting was made by means of tests and empirical rules).\n",
    "\n",
    "For determining the best model, the F1 metric with weighted average is used, due to the slight imbalance of the dataset with respect to the target (`NObeyesdad`), as described in the [Exploratory Data Analysis](../ExploratoryDataAnalysis/ExploratoryDataAnalysis.ipynb). Finally, the best model obtained during training is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining optimizer, loss function and metric\n",
    "\n",
    "_LearningRate = 1e-2\n",
    "Optimizer = partial(optim.Adam,lr=_LearningRate)\n",
    "\n",
    "LossFunction = nn.CrossEntropyLoss()\n",
    "MetricFunction = src.F1_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing trainer of Neural Network\n",
    "\n",
    "NN_Trainer = src.NeuralNetworTrainer(\n",
    "        NeuralNetwork,\n",
    "        Optimizer,\n",
    "        LossFunction,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Epoch 1 --------\n",
      "Loss :: 3.106110  [   32/ 1350]\n",
      "Loss :: 1.897667  [  352/ 1350]\n",
      "Loss :: 1.844657  [  672/ 1350]\n",
      "Loss :: 1.766185  [  992/ 1350]\n",
      "Loss :: 1.600338  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 17.6%, Avg loss: 1.597215 \n",
      "\n",
      "-------- Epoch 2 --------\n",
      "Loss :: 1.542831  [   32/ 1350]\n",
      "Loss :: 1.337827  [  352/ 1350]\n",
      "Loss :: 1.352473  [  672/ 1350]\n",
      "Loss :: 1.383006  [  992/ 1350]\n",
      "Loss :: 1.178139  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 48.0%, Avg loss: 1.172976 \n",
      "\n",
      "-------- Epoch 3 --------\n",
      "Loss :: 1.128736  [   32/ 1350]\n",
      "Loss :: 1.122481  [  352/ 1350]\n",
      "Loss :: 0.976173  [  672/ 1350]\n",
      "Loss :: 1.212871  [  992/ 1350]\n",
      "Loss :: 0.991295  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 59.6%, Avg loss: 0.927245 \n",
      "\n",
      "-------- Epoch 4 --------\n",
      "Loss :: 0.973949  [   32/ 1350]\n",
      "Loss :: 1.019642  [  352/ 1350]\n",
      "Loss :: 1.013087  [  672/ 1350]\n",
      "Loss :: 1.016852  [  992/ 1350]\n",
      "Loss :: 0.911138  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 61.7%, Avg loss: 0.853728 \n",
      "\n",
      "-------- Epoch 5 --------\n",
      "Loss :: 1.026529  [   32/ 1350]\n",
      "Loss :: 0.847899  [  352/ 1350]\n",
      "Loss :: 0.824653  [  672/ 1350]\n",
      "Loss :: 0.932220  [  992/ 1350]\n",
      "Loss :: 0.882367  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 53.1%, Avg loss: 1.113660 \n",
      "\n",
      "-------- Epoch 6 --------\n",
      "Loss :: 0.920806  [   32/ 1350]\n",
      "Loss :: 0.588147  [  352/ 1350]\n",
      "Loss :: 1.244020  [  672/ 1350]\n",
      "Loss :: 0.670094  [  992/ 1350]\n",
      "Loss :: 0.819547  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 71.1%, Avg loss: 0.687988 \n",
      "\n",
      "-------- Epoch 7 --------\n",
      "Loss :: 0.552519  [   32/ 1350]\n",
      "Loss :: 0.782962  [  352/ 1350]\n",
      "Loss :: 0.520769  [  672/ 1350]\n",
      "Loss :: 0.650218  [  992/ 1350]\n",
      "Loss :: 0.668834  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 56.7%, Avg loss: 0.965219 \n",
      "\n",
      "-------- Epoch 8 --------\n",
      "Loss :: 1.148528  [   32/ 1350]\n",
      "Loss :: 0.637033  [  352/ 1350]\n",
      "Loss :: 0.542164  [  672/ 1350]\n",
      "Loss :: 0.655870  [  992/ 1350]\n",
      "Loss :: 0.594381  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 68.7%, Avg loss: 0.654496 \n",
      "\n",
      "-------- Epoch 9 --------\n",
      "Loss :: 0.598439  [   32/ 1350]\n",
      "Loss :: 0.783080  [  352/ 1350]\n",
      "Loss :: 0.609537  [  672/ 1350]\n",
      "Loss :: 0.688592  [  992/ 1350]\n",
      "Loss :: 1.072848  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 66.1%, Avg loss: 0.732349 \n",
      "\n",
      "-------- Epoch 10 -------\n",
      "Loss :: 0.842702  [   32/ 1350]\n",
      "Loss :: 0.572824  [  352/ 1350]\n",
      "Loss :: 0.512630  [  672/ 1350]\n",
      "Loss :: 0.561163  [  992/ 1350]\n",
      "Loss :: 0.773078  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 69.7%, Avg loss: 0.735914 \n",
      "\n",
      "-------- Epoch 11 -------\n",
      "Loss :: 0.715422  [   32/ 1350]\n",
      "Loss :: 0.517683  [  352/ 1350]\n",
      "Loss :: 0.565472  [  672/ 1350]\n",
      "Loss :: 0.696752  [  992/ 1350]\n",
      "Loss :: 0.379375  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 69.2%, Avg loss: 0.614727 \n",
      "\n",
      "-------- Epoch 12 -------\n",
      "Loss :: 0.595692  [   32/ 1350]\n",
      "Loss :: 1.035139  [  352/ 1350]\n",
      "Loss :: 0.846876  [  672/ 1350]\n",
      "Loss :: 0.581484  [  992/ 1350]\n",
      "Loss :: 0.551471  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 73.0%, Avg loss: 0.557227 \n",
      "\n",
      "-------- Epoch 13 -------\n",
      "Loss :: 0.441786  [   32/ 1350]\n",
      "Loss :: 0.339835  [  352/ 1350]\n",
      "Loss :: 0.739959  [  672/ 1350]\n",
      "Loss :: 0.524810  [  992/ 1350]\n",
      "Loss :: 0.522753  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 77.5%, Avg loss: 0.515672 \n",
      "\n",
      "-------- Epoch 14 -------\n",
      "Loss :: 0.325852  [   32/ 1350]\n",
      "Loss :: 0.412133  [  352/ 1350]\n",
      "Loss :: 0.484334  [  672/ 1350]\n",
      "Loss :: 0.474945  [  992/ 1350]\n",
      "Loss :: 0.300622  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 76.8%, Avg loss: 0.546040 \n",
      "\n",
      "-------- Epoch 15 -------\n",
      "Loss :: 0.395388  [   32/ 1350]\n",
      "Loss :: 0.395940  [  352/ 1350]\n",
      "Loss :: 0.368664  [  672/ 1350]\n",
      "Loss :: 0.544198  [  992/ 1350]\n",
      "Loss :: 0.291741  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 76.1%, Avg loss: 0.553999 \n",
      "\n",
      "-------- Epoch 16 -------\n",
      "Loss :: 0.393352  [   32/ 1350]\n",
      "Loss :: 0.666208  [  352/ 1350]\n",
      "Loss :: 0.914664  [  672/ 1350]\n",
      "Loss :: 0.371774  [  992/ 1350]\n",
      "Loss :: 0.452392  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 80.6%, Avg loss: 0.454782 \n",
      "\n",
      "-------- Epoch 17 -------\n",
      "Loss :: 0.299126  [   32/ 1350]\n",
      "Loss :: 0.382897  [  352/ 1350]\n",
      "Loss :: 0.344315  [  672/ 1350]\n",
      "Loss :: 0.372676  [  992/ 1350]\n",
      "Loss :: 0.556469  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 83.4%, Avg loss: 0.440374 \n",
      "\n",
      "-------- Epoch 18 -------\n",
      "Loss :: 0.518755  [   32/ 1350]\n",
      "Loss :: 0.436113  [  352/ 1350]\n",
      "Loss :: 0.298366  [  672/ 1350]\n",
      "Loss :: 0.589401  [  992/ 1350]\n",
      "Loss :: 0.293902  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 74.1%, Avg loss: 0.546654 \n",
      "\n",
      "-------- Epoch 19 -------\n",
      "Loss :: 0.563665  [   32/ 1350]\n",
      "Loss :: 0.362241  [  352/ 1350]\n",
      "Loss :: 0.733198  [  672/ 1350]\n",
      "Loss :: 0.404308  [  992/ 1350]\n",
      "Loss :: 0.356063  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 86.8%, Avg loss: 0.384549 \n",
      "\n",
      "-------- Epoch 20 -------\n",
      "Loss :: 0.271257  [   32/ 1350]\n",
      "Loss :: 0.377894  [  352/ 1350]\n",
      "Loss :: 0.398837  [  672/ 1350]\n",
      "Loss :: 0.423734  [  992/ 1350]\n",
      "Loss :: 0.511647  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 86.8%, Avg loss: 0.360922 \n",
      "\n",
      "-------- Epoch 21 -------\n",
      "Loss :: 0.280722  [   32/ 1350]\n",
      "Loss :: 0.207171  [  352/ 1350]\n",
      "Loss :: 0.335923  [  672/ 1350]\n",
      "Loss :: 0.281035  [  992/ 1350]\n",
      "Loss :: 0.337094  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 85.4%, Avg loss: 0.374568 \n",
      "\n",
      "-------- Epoch 22 -------\n",
      "Loss :: 0.308073  [   32/ 1350]\n",
      "Loss :: 0.295664  [  352/ 1350]\n",
      "Loss :: 0.396806  [  672/ 1350]\n",
      "Loss :: 0.636394  [  992/ 1350]\n",
      "Loss :: 0.421183  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 83.5%, Avg loss: 0.406790 \n",
      "\n",
      "-------- Epoch 23 -------\n",
      "Loss :: 0.246840  [   32/ 1350]\n",
      "Loss :: 0.341724  [  352/ 1350]\n",
      "Loss :: 0.203243  [  672/ 1350]\n",
      "Loss :: 0.288600  [  992/ 1350]\n",
      "Loss :: 0.309349  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 87.3%, Avg loss: 0.366160 \n",
      "\n",
      "-------- Epoch 24 -------\n",
      "Loss :: 0.475936  [   32/ 1350]\n",
      "Loss :: 0.275567  [  352/ 1350]\n",
      "Loss :: 0.399250  [  672/ 1350]\n",
      "Loss :: 0.510741  [  992/ 1350]\n",
      "Loss :: 0.316158  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 89.3%, Avg loss: 0.318165 \n",
      "\n",
      "-------- Epoch 25 -------\n",
      "Loss :: 0.279800  [   32/ 1350]\n",
      "Loss :: 0.407384  [  352/ 1350]\n",
      "Loss :: 0.366965  [  672/ 1350]\n",
      "Loss :: 0.290524  [  992/ 1350]\n",
      "Loss :: 0.265564  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 84.3%, Avg loss: 0.372169 \n",
      "\n",
      "-------- Epoch 26 -------\n",
      "Loss :: 0.618947  [   32/ 1350]\n",
      "Loss :: 0.268958  [  352/ 1350]\n",
      "Loss :: 0.327588  [  672/ 1350]\n",
      "Loss :: 0.439616  [  992/ 1350]\n",
      "Loss :: 0.409051  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 87.9%, Avg loss: 0.308294 \n",
      "\n",
      "-------- Epoch 27 -------\n",
      "Loss :: 0.183496  [   32/ 1350]\n",
      "Loss :: 0.260680  [  352/ 1350]\n",
      "Loss :: 0.127701  [  672/ 1350]\n",
      "Loss :: 0.250245  [  992/ 1350]\n",
      "Loss :: 0.282003  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 87.6%, Avg loss: 0.314586 \n",
      "\n",
      "-------- Epoch 28 -------\n",
      "Loss :: 0.326388  [   32/ 1350]\n",
      "Loss :: 0.513080  [  352/ 1350]\n",
      "Loss :: 0.150978  [  672/ 1350]\n",
      "Loss :: 0.185619  [  992/ 1350]\n",
      "Loss :: 0.191620  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 91.5%, Avg loss: 0.271079 \n",
      "\n",
      "-------- Epoch 29 -------\n",
      "Loss :: 0.278740  [   32/ 1350]\n",
      "Loss :: 0.367047  [  352/ 1350]\n",
      "Loss :: 0.441306  [  672/ 1350]\n",
      "Loss :: 0.243710  [  992/ 1350]\n",
      "Loss :: 0.125580  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 90.6%, Avg loss: 0.274054 \n",
      "\n",
      "-------- Epoch 30 -------\n",
      "Loss :: 0.164324  [   32/ 1350]\n",
      "Loss :: 0.188534  [  352/ 1350]\n",
      "Loss :: 0.341547  [  672/ 1350]\n",
      "Loss :: 0.170819  [  992/ 1350]\n",
      "Loss :: 0.123209  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 77.5%, Avg loss: 0.529481 \n",
      "\n",
      "-------- Epoch 31 -------\n",
      "Loss :: 0.415943  [   32/ 1350]\n",
      "Loss :: 0.168495  [  352/ 1350]\n",
      "Loss :: 0.422278  [  672/ 1350]\n",
      "Loss :: 0.426746  [  992/ 1350]\n",
      "Loss :: 0.212850  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 81.4%, Avg loss: 0.364341 \n",
      "\n",
      "-------- Epoch 32 -------\n",
      "Loss :: 0.175615  [   32/ 1350]\n",
      "Loss :: 0.645941  [  352/ 1350]\n",
      "Loss :: 0.571418  [  672/ 1350]\n",
      "Loss :: 0.328200  [  992/ 1350]\n",
      "Loss :: 0.240868  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 83.2%, Avg loss: 0.422372 \n",
      "\n",
      "-------- Epoch 33 -------\n",
      "Loss :: 0.247042  [   32/ 1350]\n",
      "Loss :: 0.468613  [  352/ 1350]\n",
      "Loss :: 0.336490  [  672/ 1350]\n",
      "Loss :: 0.462949  [  992/ 1350]\n",
      "Loss :: 0.172379  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 83.1%, Avg loss: 0.381550 \n",
      "\n",
      "-------- Epoch 34 -------\n",
      "Loss :: 0.279865  [   32/ 1350]\n",
      "Loss :: 0.450553  [  352/ 1350]\n",
      "Loss :: 0.493206  [  672/ 1350]\n",
      "Loss :: 0.318808  [  992/ 1350]\n",
      "Loss :: 0.193317  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 87.8%, Avg loss: 0.269668 \n",
      "\n",
      "-------- Epoch 35 -------\n",
      "Loss :: 0.187059  [   32/ 1350]\n",
      "Loss :: 0.228171  [  352/ 1350]\n",
      "Loss :: 0.185411  [  672/ 1350]\n",
      "Loss :: 0.345703  [  992/ 1350]\n",
      "Loss :: 0.181123  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 89.9%, Avg loss: 0.275398 \n",
      "\n",
      "-------- Epoch 36 -------\n",
      "Loss :: 0.240320  [   32/ 1350]\n",
      "Loss :: 0.170335  [  352/ 1350]\n",
      "Loss :: 0.235936  [  672/ 1350]\n",
      "Loss :: 0.258290  [  992/ 1350]\n",
      "Loss :: 0.173498  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 91.9%, Avg loss: 0.233336 \n",
      "\n",
      "-------- Epoch 37 -------\n",
      "Loss :: 0.180645  [   32/ 1350]\n",
      "Loss :: 0.137451  [  352/ 1350]\n",
      "Loss :: 0.153128  [  672/ 1350]\n",
      "Loss :: 0.408349  [  992/ 1350]\n",
      "Loss :: 0.286545  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 89.9%, Avg loss: 0.272173 \n",
      "\n",
      "-------- Epoch 38 -------\n",
      "Loss :: 0.209740  [   32/ 1350]\n",
      "Loss :: 0.359414  [  352/ 1350]\n",
      "Loss :: 0.455961  [  672/ 1350]\n",
      "Loss :: 0.402492  [  992/ 1350]\n",
      "Loss :: 0.566393  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 74.9%, Avg loss: 0.597436 \n",
      "\n",
      "-------- Epoch 39 -------\n",
      "Loss :: 0.571247  [   32/ 1350]\n",
      "Loss :: 0.786134  [  352/ 1350]\n",
      "Loss :: 0.162892  [  672/ 1350]\n",
      "Loss :: 0.403868  [  992/ 1350]\n",
      "Loss :: 1.015521  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 78.8%, Avg loss: 0.445139 \n",
      "\n",
      "-------- Epoch 40 -------\n",
      "Loss :: 0.631084  [   32/ 1350]\n",
      "Loss :: 0.248330  [  352/ 1350]\n",
      "Loss :: 0.159449  [  672/ 1350]\n",
      "Loss :: 0.209672  [  992/ 1350]\n",
      "Loss :: 0.206875  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 88.0%, Avg loss: 0.322112 \n",
      "\n",
      "-------- Epoch 41 -------\n",
      "Loss :: 0.348883  [   32/ 1350]\n",
      "Loss :: 0.192800  [  352/ 1350]\n",
      "Loss :: 0.188559  [  672/ 1350]\n",
      "Loss :: 0.080650  [  992/ 1350]\n",
      "Loss :: 0.250678  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 88.6%, Avg loss: 0.295045 \n",
      "\n",
      "-------- Epoch 42 -------\n",
      "Loss :: 0.110304  [   32/ 1350]\n",
      "Loss :: 0.177201  [  352/ 1350]\n",
      "Loss :: 0.106228  [  672/ 1350]\n",
      "Loss :: 0.348915  [  992/ 1350]\n",
      "Loss :: 0.195404  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 87.5%, Avg loss: 0.273489 \n",
      "\n",
      "-------- Epoch 43 -------\n",
      "Loss :: 0.185432  [   32/ 1350]\n",
      "Loss :: 0.140599  [  352/ 1350]\n",
      "Loss :: 0.064860  [  672/ 1350]\n",
      "Loss :: 0.155933  [  992/ 1350]\n",
      "Loss :: 0.158519  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 89.7%, Avg loss: 0.258979 \n",
      "\n",
      "-------- Epoch 44 -------\n",
      "Loss :: 0.220191  [   32/ 1350]\n",
      "Loss :: 0.031551  [  352/ 1350]\n",
      "Loss :: 0.141919  [  672/ 1350]\n",
      "Loss :: 0.420284  [  992/ 1350]\n",
      "Loss :: 0.123092  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 91.4%, Avg loss: 0.214271 \n",
      "\n",
      "-------- Epoch 45 -------\n",
      "Loss :: 0.222252  [   32/ 1350]\n",
      "Loss :: 0.163633  [  352/ 1350]\n",
      "Loss :: 0.332456  [  672/ 1350]\n",
      "Loss :: 0.134487  [  992/ 1350]\n",
      "Loss :: 0.401865  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 91.4%, Avg loss: 0.216771 \n",
      "\n",
      "-------- Epoch 46 -------\n",
      "Loss :: 0.129408  [   32/ 1350]\n",
      "Loss :: 0.202426  [  352/ 1350]\n",
      "Loss :: 0.154127  [  672/ 1350]\n",
      "Loss :: 0.060273  [  992/ 1350]\n",
      "Loss :: 0.426734  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 92.3%, Avg loss: 0.213502 \n",
      "\n",
      "-------- Epoch 47 -------\n",
      "Loss :: 0.123909  [   32/ 1350]\n",
      "Loss :: 0.108093  [  352/ 1350]\n",
      "Loss :: 0.203191  [  672/ 1350]\n",
      "Loss :: 0.207347  [  992/ 1350]\n",
      "Loss :: 0.155754  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 91.1%, Avg loss: 0.213103 \n",
      "\n",
      "-------- Epoch 48 -------\n",
      "Loss :: 0.117129  [   32/ 1350]\n",
      "Loss :: 0.268522  [  352/ 1350]\n",
      "Loss :: 0.096968  [  672/ 1350]\n",
      "Loss :: 0.090020  [  992/ 1350]\n",
      "Loss :: 0.068732  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 90.3%, Avg loss: 0.255764 \n",
      "\n",
      "-------- Epoch 49 -------\n",
      "Loss :: 0.280730  [   32/ 1350]\n",
      "Loss :: 0.221165  [  352/ 1350]\n",
      "Loss :: 0.358257  [  672/ 1350]\n",
      "Loss :: 0.132720  [  992/ 1350]\n",
      "Loss :: 0.305747  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 88.1%, Avg loss: 0.279482 \n",
      "\n",
      "-------- Epoch 50 -------\n",
      "Loss :: 0.281385  [   32/ 1350]\n",
      "Loss :: 0.071051  [  352/ 1350]\n",
      "Loss :: 0.114414  [  672/ 1350]\n",
      "Loss :: 0.252911  [  992/ 1350]\n",
      "Loss :: 0.102293  [ 1312/ 1350]\n",
      "Test Error: \n",
      "Score: 88.2%, Avg loss: 0.318762 \n",
      "\n",
      "Best Score: 92.3%\n"
     ]
    }
   ],
   "source": [
    "# Training of neural network\n",
    "\n",
    "_Epochs = 50\n",
    "BatchSize = 32\n",
    "BestModel = NN_Trainer(\n",
    "    Dataset_Train,\n",
    "    Dataset_Evaluation,\n",
    "    BatchSize,\n",
    "    _Epochs,\n",
    "    MetricFunction,\n",
    "    TORCH_DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best model\n",
    "\n",
    "# src.SaveModelNN(BestModel,PATH_SAVE,'NN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
